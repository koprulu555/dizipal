#!/usr/bin/env python3
"""
Dizipal M3U OluÅŸturucu
AmacÄ±: Dizipal sitesindeki tÃ¼m film/dizi sayfa URL'lerini toplar.
Ã‡Ä±ktÄ±: dizipal.m3u (Ä°Ã§inde M3U8 deÄŸil, site iÃ§i sayfa baÄŸlantÄ±larÄ± bulunur)
"""

import cloudscraper
import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urljoin, urlparse

class DizipalScraper:
    def __init__(self):
        # 1. ADIM: GÃ¼ncel domain'i al
        self.base_url = self.get_current_domain()
        print(f"ğŸ“ Ã‡alÄ±ÅŸÄ±lan Site: {self.base_url}")
        self.scraper = cloudscraper.create_scraper()
        self.scraper.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.all_urls = set()  # Tekrar eden URL'leri engellemek iÃ§in

    def get_current_domain(self):
        """GitHub'dan gÃ¼ncel domain'i Ã§eker."""
        try:
            domain_url = "https://raw.githubusercontent.com/koprulu555/domain-kontrol2/refs/heads/main/dizipaldomain.txt"
            response = requests.get(domain_url, timeout=10)
            for line in response.text.splitlines():
                if line.startswith("guncel_domain="):
                    domain = line.split('=', 1)[1].strip()
                    if domain:
                        return domain.rstrip('/')
        except Exception as e:
            print(f"âš ï¸  Domain alÄ±nÄ±rken hata: {e}")
        
        # Yedek domain (proje aÃ§Ä±klamasÄ±nda verilen)
        return "https://dizipal1222.com"

    def get_sitemap_urls(self):
        """Ana sitemap.xml'den tÃ¼m iÃ§erik URL'lerini toplar."""
        sitemap_url = urljoin(self.base_url, "/sitemap.xml")
        print(f"ğŸ—ºï¸  Site haritasÄ± taranÄ±yor: {sitemap_url}")
        
        try:
            resp = self.scraper.get(sitemap_url, timeout=30)
            soup = BeautifulSoup(resp.content, 'lxml-xml')
            urls = []
            
            # Sitemap iÃ§indeki <loc> etiketlerini bul
            for loc in soup.find_all('loc'):
                url = loc.text.strip()
                if self.base_url in url:
                    urls.append(url)
            
            print(f"âœ… Site haritasÄ±ndan {len(urls)} URL bulundu.")
            return urls
        except Exception as e:
            print(f"âŒ Site haritasÄ± alÄ±namadÄ±: {e}")
            return []

    def classify_and_filter_urls(self, urls):
        """URL'leri kategorilere ayÄ±rÄ±r ve filtreler."""
        categories = {
            'diziler': [],
            'filmler': [],
            'platforms': {}
        }
        
        # Platform listesi (senin belirttiÄŸin gibi)
        platform_keywords = {
            'netflix': 'NETFLÄ°X',
            'exxen': 'GAIN',
            'blutv': 'BluTV',
            'disney': 'Disney+',
            'amazon-prime': 'Amazon Prime',
            'tod-bein': 'TOD',
            'gain': 'GAIN',
            'mubi': 'Mubi'
        }
        
        for url in urls:
            # 1. Dizi bÃ¶lÃ¼mlerini bul (sezon/bolum pattern'i)
            if '/dizi/' in url and '/sezon-' in url and '/bolum-' in url:
                categories['diziler'].append(url)
            
            # 2. Film sayfalarÄ±nÄ± bul (film/ ile baÅŸlayan veya film- iÃ§eren)
            elif '/film/' in url or '/film-' in url:
                categories['filmler'].append(url)
            
            # 3. Platform koleksiyonlarÄ±nÄ± bul
            for keyword, platform_name in platform_keywords.items():
                if keyword in url:
                    if platform_name not in categories['platforms']:
                        categories['platforms'][platform_name] = []
                    categories['platforms'][platform_name].append(url)
                    break
        
        print(f"ğŸ“Š SÄ±nÄ±flandÄ±rma: {len(categories['diziler'])} dizi, {len(categories['filmler'])} film")
        for platform, links in categories['platforms'].items():
            print(f"   - {platform}: {len(links)} iÃ§erik")
        
        return categories

    def generate_m3u_content(self, categories):
        """Kategorilerden M3U iÃ§eriÄŸi oluÅŸturur."""
        m3u_lines = ['#EXTM3U']
        
        # DÄ°ZÄ°LER
        m3u_lines.append('\n# KATEGORI: DÄ°ZÄ°LER')
        for url in sorted(categories['diziler'])[:500]:  # Ä°lk 500'Ã¼ al (sÄ±nÄ±rlama)
            # Dizi adÄ±nÄ± URL'den Ã§Ä±kar
            name = self.extract_name_from_url(url)
            m3u_lines.append(f'#EXTINF:-1, {name}')
            m3u_lines.append(url)
        
        # FÄ°LMLER
        m3u_lines.append('\n# KATEGORI: FÄ°LMLER')
        for url in sorted(categories['filmler'])[:300]:
            name = self.extract_name_from_url(url)
            m3u_lines.append(f'#EXTINF:-1, {name}')
            m3u_lines.append(url)
        
        # PLATFORMLAR
        for platform_name, urls in sorted(categories['platforms'].items()):
            m3u_lines.append(f'\n# KATEGORI: {platform_name.upper()}')
            for url in urls[:100]:
                name = self.extract_name_from_url(url)
                m3u_lines.append(f'#EXTINF:-1, {name}')
                m3u_lines.append(url)
        
        return '\n'.join(m3u_lines)

    def extract_name_from_url(self, url):
        """URL'den insanlarÄ±n okuyabileceÄŸi bir isim Ã§Ä±karÄ±r."""
        # Ã–rnek: https://dizipal1222.com/dizi/enfes-bir-aksam/sezon-1/bolum-1
        # Ã‡Ä±ktÄ±: Enfes Bir Aksam S01E01
        
        parsed = urlparse(url)
        path = parsed.path
        
        # Dizi bÃ¶lÃ¼mÃ¼ iÃ§in
        if '/sezon-' in path and '/bolum-' in path:
            match = re.search(r'/dizi/([^/]+)/sezon-(\d+)/bolum-(\d+)', path)
            if match:
                name = match.group(1).replace('-', ' ').title()
                season = match.group(2).zfill(2)
                episode = match.group(3).zfill(2)
                return f"{name} S{season}E{episode}"
        
        # Film veya dizi ana sayfasÄ± iÃ§in
        match = re.search(r'/(?:dizi|film)/([^/]+)', path)
        if match:
            name = match.group(1).replace('-', ' ').title()
            return name
        
        return "Ä°simsiz Ä°Ã§erik"

    def run(self):
        """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu."""
        print("ğŸš€ Dizipal M3U OluÅŸturucu BaÅŸlÄ±yor...")
        
        # 1. Sitemap'ten URL'leri al
        all_urls = self.get_sitemap_urls()
        if not all_urls:
            print("âŒ Site haritasÄ± boÅŸ, alternatif yÃ¶ntem deneniyor...")
            # Alternatif yÃ¶ntem eklenebilir
            return
        
        # 2. URL'leri sÄ±nÄ±flandÄ±r
        categories = self.classify_and_filter_urls(all_urls)
        
        # 3. M3U iÃ§eriÄŸini oluÅŸtur
        m3u_content = self.generate_m3u_content(categories)
        
        # 4. Dosyaya yaz
        with open('dizipal.m3u', 'w', encoding='utf-8') as f:
            f.write(m3u_content)
        
        print(f"âœ… Ä°ÅŸlem tamam! Toplam {len(m3u_content.splitlines())} satÄ±r yazÄ±ldÄ±.")
        print(f"ğŸ“ Ã‡Ä±ktÄ±: dizipal.m3u")

if __name__ == "__main__":
    scraper = DizipalScraper()
    scraper.run()
