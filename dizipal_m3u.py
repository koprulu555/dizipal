#!/usr/bin/env python3
"""
DÄ°ZÄ°PAL M3U OLUÅTURUCU
Sadece sayfa URL'lerini toplar: https://dizipal1222.com/dizi/enfes-bir-aksam/sezon-1/bolum-1
"""

import cloudscraper
import requests
import re
import time
from urllib.parse import urljoin

class DizipalScraper:
    def __init__(self):
        self.base_url = self.get_current_domain()
        print(f"ğŸ”— Domain: {self.base_url}")
        self.scraper = cloudscraper.create_scraper()
        self.scraper.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.all_links = []
        self.platforms = {
            'netflix': 'NETFLÄ°X',
            'exxen': 'GAIN', 
            'blutv': 'BluTV',
            'disney': 'Disney+',
            'amazon-prime': 'Amazon Prime',
            'tod-bein': 'TOD',
            'gain': 'GAIN',
            'mubi': 'Mubi'
        }

    def get_current_domain(self):
        """GitHub'dan gÃ¼ncel domain'i al"""
        try:
            url = "https://raw.githubusercontent.com/koprulu555/domain-kontrol2/refs/heads/main/dizipaldomain.txt"
            r = requests.get(url, timeout=10)
            for line in r.text.split('\n'):
                if line.startswith('guncel_domain='):
                    domain = line.split('=', 1)[1].strip()
                    if domain:
                        return domain.rstrip('/')
        except:
            pass
        return "https://dizipal1222.com"

    def get_sitemap(self):
        """Sitemap'ten tÃ¼m linkleri Ã§ek"""
        sitemap_url = f"{self.base_url}/sitemap.xml"
        print(f"ğŸ“„ Sitemap: {sitemap_url}")
        
        try:
            r = self.scraper.get(sitemap_url, timeout=30)
            links = re.findall(r'<loc>(.*?)</loc>', r.text)
            return [link for link in links if self.base_url in link]
        except Exception as e:
            print(f"âŒ Sitemap hatasÄ±: {e}")
            return []

    def crawl_category(self, url, category_name):
        """Kategori sayfasÄ±ndaki tÃ¼m iÃ§erikleri bul"""
        print(f"ğŸ” {category_name} taranÄ±yor: {url}")
        
        try:
            r = self.scraper.get(url, timeout=30)
            
            # Dizi linklerini bul (senin yapÄ±na gÃ¶re)
            dizi_links = re.findall(r'href="(/dizi/[^"]+)"', r.text)
            for link in dizi_links:
                if '/sezon-' in link and '/bolum-' in link:
                    full_url = urljoin(self.base_url, link)
                    if full_url not in self.all_links:
                        self.all_links.append(full_url)
            
            # Film linklerini bul
            film_links = re.findall(r'href="(/film/[^"]+)"', r.text)
            for link in film_links:
                full_url = urljoin(self.base_url, link)
                if full_url not in self.all_links:
                    self.all_links.append(full_url)
                    
        except Exception as e:
            print(f"âš ï¸  {category_name} hatasÄ±: {e}")

    def organize_links(self):
        """Linkleri kategorilere ayÄ±r"""
        categories = {
            'DÄ°ZÄ°LER': [],
            'FÄ°LMLER': [],
            'PLATFORMLAR': {}
        }
        
        for link in self.all_links:
            # Platform kontrolÃ¼
            platform_found = False
            for key, name in self.platforms.items():
                if key in link:
                    if name not in categories['PLATFORMLAR']:
                        categories['PLATFORMLAR'][name] = []
                    categories['PLATFORMLAR'][name].append(link)
                    platform_found = True
                    break
            
            if not platform_found:
                if '/dizi/' in link:
                    categories['DÄ°ZÄ°LER'].append(link)
                elif '/film/' in link:
                    categories['FÄ°LMLER'].append(link)
        
        return categories

    def generate_m3u(self, categories):
        """M3U dosyasÄ±nÄ± oluÅŸtur"""
        m3u_content = ["#EXTM3U"]
        
        # DÄ°ZÄ°LER
        m3u_content.append("\n# KATEGORÄ°: DÄ°ZÄ°LER")
        for url in sorted(categories['DÄ°ZÄ°LER'])[:1000]:  # Ä°lk 1000
            name = self.extract_name(url)
            m3u_content.append(f"#EXTINF:-1, {name}")
            m3u_content.append(url)
        
        # FÄ°LMLER
        m3u_content.append("\n# KATEGORÄ°: FÄ°LMLER")
        for url in sorted(categories['FÄ°LMLER'])[:500]:
            name = self.extract_name(url)
            m3u_content.append(f"#EXTINF:-1, {name}")
            m3u_content.append(url)
        
        # PLATFORMLAR
        for platform, urls in categories['PLATFORMLAR'].items():
            m3u_content.append(f"\n# KATEGORÄ°: {platform}")
            for url in urls[:200]:
                name = self.extract_name(url)
                m3u_content.append(f"#EXTINF:-1, {name}")
                m3u_content.append(url)
        
        return "\n".join(m3u_content)

    def extract_name(self, url):
        """URL'den isim Ã§Ä±kar"""
        # Ã–rnek: /dizi/enfes-bir-aksam/sezon-1/bolum-1
        match = re.search(r'/(dizi|film)/([^/]+)', url)
        if match:
            name = match.group(2).replace('-', ' ').title()
            
            # Sezon/bÃ¶lÃ¼m bilgisi
            season_match = re.search(r'/sezon-(\d+)', url)
            episode_match = re.search(r'/bolum-(\d+)', url)
            
            if season_match and episode_match:
                return f"{name} S{season_match.group(1).zfill(2)}E{episode_match.group(1).zfill(2)}"
            return name
        return "Ä°simsiz"

    def run(self):
        """Ana fonksiyon"""
        print("ğŸš€ Dizipal M3U OluÅŸturucu BaÅŸlÄ±yor...\n")
        
        # 1. Sitemap'ten link al
        sitemap_links = self.get_sitemap()
        
        # 2. Kategorileri tara
        categories_to_crawl = [
            (f"{self.base_url}/diziler", "Diziler"),
            (f"{self.base_url}/filmler", "Filmler"),
            (f"{self.base_url}/diziler/son-bolumler", "Son BÃ¶lÃ¼mler"),
        ]
        
        # PlatformlarÄ± ekle
        for platform in self.platforms.keys():
            categories_to_crawl.append((f"{self.base_url}/koleksiyon/{platform}", platform))
        
        for url, name in categories_to_crawl:
            self.crawl_category(url, name)
            time.sleep(1)  # Sunucuyu yormamak iÃ§in
        
        # 3. Linkleri dÃ¼zenle
        categories = self.organize_links()
        
        print(f"\nğŸ“Š BULUNANLAR:")
        print(f"   Diziler: {len(categories['DÄ°ZÄ°LER'])}")
        print(f"   Filmler: {len(categories['FÄ°LMLER'])}")
        for platform, urls in categories['PLATFORMLAR'].items():
            print(f"   {platform}: {len(urls)}")
        
        # 4. M3U oluÅŸtur
        m3u_content = self.generate_m3u(categories)
        
        # 5. Dosyaya yaz
        with open('dizipal.m3u', 'w', encoding='utf-8') as f:
            f.write(m3u_content)
        
        print(f"\nâœ… BAÅARILI! {len(m3u_content.splitlines())} satÄ±r yazÄ±ldÄ±.")
        print(f"ğŸ“ Dosya: dizipal.m3u")

if __name__ == "__main__":
    scraper = DizipalScraper()
    scraper.run()
